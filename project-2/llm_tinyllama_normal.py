# -*- coding: utf-8 -*-
"""LLM-TinyLlama-Normal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1btTPLVAqZ_7L27ouOrbf_IPhSo9eLC9J
"""

!pip install transformers torch accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True
)

def chat(user_message):
    # Format as chat
    chat = [
        {"role": "user", "content": user_message}
    ]

    # Apply chat template
    formatted_prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True
    )

    # Generate response
    inputs = tokenizer.encode(formatted_prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )

    response_tokens = outputs[0][inputs.shape[1]:]
    response = tokenizer.decode(response_tokens, skip_special_tokens=True)

    return response.strip()

user_question = "What team won the 2014 world cup?"
chat_response = chat(user_question)
print(f"User: {user_question}")
print(f"Assistant: {chat_response}")